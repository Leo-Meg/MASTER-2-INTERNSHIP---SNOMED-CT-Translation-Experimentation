# MASTER-2-INTERNSHIP---SNOMED-CT-Translation-Experimentation

# NRCâ€‘traductionâ€‘snomedâ€‘LLM

> **Fewâ€‘shot translation of SNOMEDÂ CT preferred terms (EnglishÂ â†’Â French / Spanish) with large language models**

---

##â€¯Table of contents

1. [Project overview](#overview)
2. [Folder layout](#layout)
3. [Prerequisites](#prereq)
4. [Installation](#install)
5. [ExecutionÂ pipeline](#pipeline)
   Â Â 5â€‘a.Â [StepÂ 0Â â€“Â build the graph](#step0)
   Â Â 5â€‘b.Â [StepÂ 1Â â€“Â Node2Vec embeddings](#step1)
   Â Â 5â€‘c.Â [StepÂ 2Â â€“Â Translation & lexical embeddings](#step2)
   Â Â 5â€‘d.Â [StepÂ 3Â â€“Â Example retrieval](#step3)
   Â Â 5â€‘e.Â [StepÂ 4Â â€“Â Fiveâ€‘shot generation](#step4)
6. [Generated artefacts](#artefacts)
7. [TroubleshootingÂ & FAQ](#faq)
8. [License & citation](#license)

 ##Â 1Â â€“Â Project overview This repository contains a **reproducible research pipeline** that compares several exampleâ€‘retrieval strategies for translating SNOMEDÂ CT concepts with an instructionâ€‘tuned multilingual LLM (Ayaâ€‘101).
The experiment:

* extracts **French** and **Spanish** preferred terms (PT) from their respective National Extensions;
* computes four families of similarity metrics (BoWÂ nâ€‘gram, TFâ€‘IDF, Sentenceâ€‘Transformer, Node2Vec);
* builds mixed graph/semantic **fewâ€‘shot prompts** (5 exemplars) for every strategy;
* asks the model to translate new English PTs and logs the outputs for further evaluation.

 ##Â 2Â â€“Â Folder layout

```
â”œâ”€â”€ data/                     # autoâ€‘created; all outputs land here
â”‚Â Â  â”œâ”€â”€ embeddings/           # *.pkl.gz embedding dictionaries
â”‚Â Â  â””â”€â”€ five_shot_generations # final TSV translations
â”œâ”€â”€ snomed_graph/             # cached GML graph of the International Edition
â”œâ”€â”€ SnomedCT_FR/              # French Extension (validated translations)
â”œâ”€â”€ SnomedCT_SpanishReleaseâ€‘es_PRODUCTION_20240930T120000Z/
â”œâ”€â”€ generate_node2vec_embeddings.py
â”œâ”€â”€ generate_node2vec.sh                   # SLURM wrapper
â”œâ”€â”€ load_translations_embedding.py          
â”œâ”€â”€ retrieval_example.py
â”œâ”€â”€ generate_five_shot.py
â””â”€â”€ snomed_pipeline_translations.sh         # SLURM wrapper for full pipeline
```

*Only the key research scripts are listed above â€“ notebooks and helper utils omitted for brevity.*

 ##Â 3Â â€“Â Prerequisites ###Â 3.1Â Datasets

| What                                                                         | Where to put it                                                                                                      | Notes                               |
| ---------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | ----------------------------------- |
| **SNOMEDÂ CT International Edition** (RF2, 2024â€‘12 release used in the paper) | *anywhere* â†’ convert once toÂ `full_concept_graph_snomed_ct_int_rf2_20241201.gml` inside `snomed_graph/` (see StepÂ 0) | required                            |
| **French validated translations**Â `traductions_validÃ©es_20241218_v2.csv`     | `SnomedCT_FR/`                                                                                                       | required                            |
| **Spanish Extension 2024â€‘09** (FullÂ +Â Snapshot folders)                      | `SnomedCT_SpanishReleaseâ€‘es_PRODUCTION_20240930T120000Z/`                                                            | required                            |
| **default_example.csv** (handâ€‘crafted seed examples in FR)                   | project root                                                                                                         | optional â€“ improves default prompts |

###Â 3.2Â Software â€“Â two Conda environments `node2vec` still relies on **NetworkXÂ <Â 3.0** while `snomed_graph` needs **NetworkXÂ â‰¥â€¯3.2**.  Mixing the two breaks both, therefore we keep them **separate**:

| Environment              | Purpose                        | Main deps                                                                          |
| ------------------------ | ------------------------------ | ---------------------------------------------------------------------------------- |
| **env_node2vec**         | graph walks & Node2Vec fitting | node2vec, networkxÂ 2.x, gensim                                                     |
| **env_snomed_trans_poc** | everything else                | snomed_graph, torch, sentenceâ€‘transformers, faissâ€‘cpu, transformers, Ayaâ€‘101, etc. |

Minimal recipes are provided in `envs/` (feel free to tweak CUDA versions).

 ##Â 4Â â€“Â Installation (oneâ€‘off)

```bash
# clone the repo
$ git clone https://github.com/<you>/NRC-traduction-snomed-LLM.git
$ cd NRC-traduction-snomed-LLM

# create both envs
$ conda env create -f envs/env_node2vec.yml
$ conda env create -f envs/env_snomed_trans_poc.yml
```

On HPC clusters using **SLURM**, the two submission scripts already activate the right environment; on a local workstation just `conda activate` them manually before each step.

 ##Â 5Â â€“Â ExecutionÂ pipeline The whole run boils down to **two commands** â€“ the first builds Node2Vec embeddings, the second drives the remainder of the experiment.

```bash
#Â STEPÂ 1 â€“Â run once (â‰ˆÂ 24Â h on 30â€¯k nodes)
$ sbatch generate_node2vec.sh           # OR: conda activate env_node2vec && python generate_node2vec_embeddings.py

#Â STEPÂ 2 â€“Â full translation pipeline (GPU recommended)
$ sbatch snomed_pipeline_translations.sh             # default (complete run)
$ sbatch snomed_pipeline_translations.sh test        # tiny dryâ€‘run
$ sbatch snomed_pipeline_translations.sh skip-embeddings  # reuse existing *.pkl.gz
```

Below is the detailed flow for curious minds.

 ###Â StepÂ 0Â â€“Â Build the concept graph *(oneâ€‘off)* If you do **not** have `snomed_graph/full_concept_graph_snomed_ct_int_rf2_20241201.gml` yet:

```python
from snomed_graph.snomed_graph import SnomedGraph
SnomedGraph.from_rf2_folder("/path/to/SnomedCT_International_RF2") \
           .to_serialized("snomed_graph/full_concept_graph_snomed_ct_int_rf2_20241201.gml")
```

This step can be executed inside **env_snomed_trans_poc**.

 ###Â StepÂ 1Â â€“Â Node2Vec embeddings *(env_node2vec)*

| Script                                                 | Inputs                                  | Outputs                                        |
| ------------------------------------------------------ | --------------------------------------- | ---------------------------------------------- |
| `generate_node2vec_embeddings.py` *(called by **``**)* | `snomed_graph/full_concept_graph_*.gml` | `data/embeddings/graph_node2vec_high_q.pkl.gz` |

Parameters (`walk_length=100`, `num_walks=20`, `dims=512`, *p*Â =Â 0.5, *q*Â =Â 2) match the notebook.

 ###Â StepÂ 2Â â€“Â Load translationsÂ & lexical embeddings *(env_snomed_trans_poc)*

| Script                           | Inputs                                         | Outputs                                                                 |
| -------------------------------- | ---------------------------------------------- | ----------------------------------------------------------------------- |
| `load_translations_embedding.py` | FrenchÂ &Â Spanish extension files, SNOMED graph | `data/all_translations.csv`                                             |
| Â                                 |                                                | `data/samples.csv`                                                      |
| Â                                 |                                                | BoW, TFâ€‘IDF, ST embeddings (`data/embeddings/*.pkl.gz` except Node2Vec) |

Flags:

* `--skip-embeddings` â†’ only refresh CSVs
* `--force` â†’ overwrite everything

 ###Â StepÂ 3Â â€“Â Retrieve inâ€‘context examples *(env_snomed_trans_poc)*

| Script                 | Inputs                                             | Outputs                                         |
| ---------------------- | -------------------------------------------------- | ----------------------------------------------- |
| `retrieval_example.py` | graph, CSVs, all embeddings, `default_example.csv` | `data/default_example_fr_es.csv`                |
| Â                       |                                                    | `data/example_retrieval_es_fr.csv` (â‰ˆÂ 2â€¯M rows) |

The file contains both *graph*â€‘based and *vector*â€‘based neighbours with scores and provenance.

 ###Â StepÂ 4Â â€“Â Generate fiveâ€‘shot promptsÂ & translations *(env_snomed_trans_poc, GPUÂ 11â€¯GBÂ +)*

| Script                                                        | Inputs                                                        | Outputs                                                                |
| ------------------------------------------------------------- | ------------------------------------------------------------- | ---------------------------------------------------------------------- |
| `generate_five_shot.py`                                       | graph, translations CSVs, example retrieval, default examples | `data/five_shot_generations/five_shot_<method>.tsv` for seven methods: |
| `bow`, `tfidf`, `enc`, `graph`, `rgraph`, `default`, `random` |                                                               |                                                                        |

Useful options:

* `--test` â€“ limit to 20 concepts per method (quick sanity check)
* `--batch-size`, `--max-new` â€“ control Ayaâ€‘101 generation

 ##Â 6Â â€“Â Generated artefacts

```
data/
â”œâ”€â”€ all_translations.csv           # 6.8â€¯M rows Ã—Â concept features
â”œâ”€â”€ samples.csv                    # balanced evaluation sample (â‰ˆâ€¯7â€¯k rows)
â”œâ”€â”€ embeddings/
â”‚Â Â  â”œâ”€â”€ bow_binary_ngram.pkl.gz
â”‚Â Â  â”œâ”€â”€ tfidf_ngram.pkl.gz
â”‚Â Â  â”œâ”€â”€ st_multilingual.pkl.gz
â”‚Â Â  â””â”€â”€ graph_node2vec_high_q.pkl.gz
â”œâ”€â”€ default_example_fr_es.csv      # seed examples enriched with ES PT
â”œâ”€â”€ example_retrieval_es_fr.csv    # every candidate neighbour + metadata
â””â”€â”€ five_shot_generations/
    â”œâ”€â”€ five_shot_bow.tsv
    â”œâ”€â”€ five_shot_tfidf.tsv
    â”œâ”€â”€ five_shot_enc.tsv
    â”œâ”€â”€ five_shot_graph.tsv
    â”œâ”€â”€ five_shot_rgraph.tsv
    â”œâ”€â”€ five_shot_default.tsv
    â””â”€â”€ five_shot_random.tsv
```

Each TSV contains: `sctid`, `preferred_term`, `aya_translation`, reference translation, prompt text, etc.

 ##Â 7Â â€“Â TroubleshootingÂ & FAQ

* **ImportError: *****No module named â€˜networkx.drawingâ€™*** â€“ you probably mixed the two environments.  Doubleâ€‘check with `conda list | grep networkx`.
* **GPU OOM during Ayaâ€‘101 generation** â€“ use `--batch-size 8` or fall back to CPU (automatic) â€“ it will just be slower.
* **Need to regenerate only TFâ€‘IDF embeddings** â€“ `python load_translations_embedding.py --force --skip-embeddings` then delete `tfidf_ngram.pkl.gz` and rerun without `--skip-embeddings`.
* **Where do results of the paper come from?** â€“ see `ANALYSIS.ipynb` for replication of all metrics.

 ##Â 8Â â€“Â License & citation Code is released under the **ApacheÂ 2.0** license.  SNOMEDÂ CT data is Â©â€¯SNOMEDÂ International and distributed under the SNOMEDÂ CT Affiliate License; you must hold a valid license to use the terminology files.

If you use this pipeline, please cite:

```text
MegretÂ L., 2025. Fewâ€‘shot Translation of SNOMEDÂ CT with Large Language Models. Inria SED, Technical Report.
```

Happy translating! ðŸŽ‰
